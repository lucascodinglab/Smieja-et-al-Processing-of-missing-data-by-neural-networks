{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyMXjy8442DpGWh3x3sDNaWY",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lucascodinglab/Smieja_Neuronal_Network/blob/main/Smieja_Neuronal_Network.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oZX6zPyi9gVy",
        "outputId": "efa1ac87-3347-45fa-d6b4-fa8ee0dbf98d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive/\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive/', force_remount=True)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import statsmodels.api as sm\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import math\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import *\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "from sklearn import preprocessing\n",
        "import statsmodels.api as sm\n",
        "import tqdm\n",
        "import sklearn\n",
        "from sklearn import datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.mixture import GaussianMixture\n",
        "from scipy.stats import multivariate_normal\n",
        "from tensorflow.python.ops.math_ops import sqrt_grad\n",
        "import tensorflow as tf\n",
        "import warnings\n",
        "import keras\n",
        "import torch\n",
        "from tensorflow.keras.utils import to_categorical"
      ],
      "metadata": {
        "id": "A8gV9nkH90GZ"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Model():\n",
        "    \n",
        "    def __init__(self, weights, biases, alpha=0.00):\n",
        "        self.weights = weights\n",
        "        self.biases = biases\n",
        "        self.num_layers = len(weights)\n",
        "        self.alpha = alpha \n",
        "    \n",
        "    #propagates a list of points through the model; each hidden layer is followed by an \n",
        "    #activation layer by a leaky ReLu with self.alpha as parameter \n",
        "    #need to write a function for PropagationBase.py which uses activation function already implemented \n",
        "    #points must be numpy arrays of size >=2\n",
        "    def propagate(self, point, start_layer):\n",
        "        for i in range(start_layer, self.num_layers):\n",
        "            point = np.matmul(self.weights[i], point) + self.biases[i]\n",
        "           \n",
        "            #if i in range(0, self.num_layers-1):\n",
        "            if i < self.num_layers - 1:\n",
        "                for j in range(0, self.weights[i].shape[0]):\n",
        "                    if point[j]<0:\n",
        "                        point[j] = self.alpha * point[j]\n",
        "        \n",
        "            \n",
        "        \n",
        "        return point\n",
        "        \n",
        "def loadModel(file, alpha=0.00):\n",
        "    \n",
        "    param_dict = torch.load(file) #\"NN1.pt\"\n",
        "    w = []\n",
        "    b= []\n",
        "    num_layers = len({int(item.split(\"_\")[1]) for item in param_dict})\n",
        "    #print(num_layers)\n",
        "    for i in range(num_layers):\n",
        "            w.append(param_dict[\"weights_\" + str(i)].numpy())\n",
        "            b.append(param_dict[\"bias_\" + str(i)].numpy())\n",
        "\n",
        "    model = Model(w, b, alpha=alpha)\n",
        "    return model \n",
        "\n",
        "def generate_missing_values(complete_data, p):\n",
        "    shape = complete_data.shape\n",
        "    y = complete_data.copy()\n",
        "    missing = np.random.binomial(1, p, shape)\n",
        "    y[missing.astype('bool')] = np.nan\n",
        "    #print(y, complete_data)\n",
        "    return y\n",
        "\n",
        "def fit_distribution(x, components_for_GMM, type_of_distribution=\"normal\"): # x sind alle daten und vollstÃ¤ndig \n",
        "    if type_of_distribution == \"normal\":\n",
        "        distr_on_x = GaussianMixture(n_components = 1, covariance_type= \"diag\").fit(x) \n",
        "    if type_of_distribution == \"GMM\":\n",
        "        distr_on_x = GaussianMixture(n_components = components_for_GMM, covariance_type= \"diag\").fit(x) \n",
        "    return distr_on_x  \n",
        "\n",
        "def nr_new(x):\n",
        "    one_diff_sqrt = np.divide(1,np.sqrt(2*math.pi))\n",
        "    exp = np.exp(-np.divide(x**2, 2))\n",
        "    erf_input = np.divide(x, np.sqrt(2))\n",
        "    erf = math.erf(erf_input)\n",
        "    erf_part = np.divide(x,2) * (1+erf)\n",
        "    return one_diff_sqrt * exp + erf_part   \n",
        "\n",
        "def relu_gmm(mu, sigma, w, p, b): # calculates relu for every neuron of the first hidden layer()\n",
        "    wT = np.transpose(w)  \n",
        "    sqrt_ = np.sqrt(np.dot(np.multiply(wT, sigma), w))\n",
        "    part_1 = p * sqrt_\n",
        "    x = np.divide(wT @ mu + b, sqrt_)\n",
        "    part_2 = nr_new(x)\n",
        "\n",
        "    relu_f = part_1 * part_2\n",
        "\n",
        "    return relu_f\n",
        "\n",
        "def regularization(gamma, n_components, cov, mu, x_certain):\n",
        "    reg_weights = []\n",
        "    num_dim_certain = len(x_certain)\n",
        "    for component in range(0,n_components):  \n",
        "        cov_component = cov[0][component]\n",
        "        mu_component = mu[component]\n",
        "        sub_part_factors = 1\n",
        "        sub_part_sum = 0\n",
        "        for dim in range(0,num_dim_certain):\n",
        "            sub_part_factors *= (gamma + cov_component[dim])**0.5  \n",
        "            sub_part_sum += ( (1 / gamma + cov_component[dim]) * (mu_component[0][dim] - x_certain[dim])**2)\n",
        "            # print(\"mu_component: \",mu_component[0][dim])\n",
        "            # print(\"x_certain: \",x_certain[dim])\n",
        "            \n",
        "            \n",
        "        part_1 = 1 / (2 * np.pi**(num_dim_certain / 2) * sub_part_factors ) \n",
        "        part_2 = math.exp(-0.5 * sub_part_sum)\n",
        "        C = part_1 * part_2\n",
        "        # print(\"sub_part_sum: \",sub_part_sum)\n",
        "        # print(\"part_2: \", part_2)\n",
        "        \n",
        "        reg_weights.append(C)\n",
        "    \n",
        "    return reg_weights\n",
        "\n",
        "def GMM_missing_generation(y, distr,n_components, gamma, type_of_distribution=\"GMM\"):\n",
        "    if type_of_distribution == \"normal\":\n",
        "        pass\n",
        "    if type_of_distribution == \"GMM\":\n",
        "        missing_values = []\n",
        "        certain_mean = []\n",
        "        certain_cov = []\n",
        "        for i, yi in enumerate(y):\n",
        "            if math.isnan(yi):\n",
        "                missing_values.append(True)\n",
        "            else:\n",
        "                missing_values.append(False)\n",
        "                certain_mean.append(yi)\n",
        "                certain_cov.append(0)\n",
        "        if True in missing_values:\n",
        "            if False in missing_values:\n",
        "\n",
        "                GMM_missing = GaussianMixture()\n",
        "                GMM_missing.means_ = np.array([[j[missing_values]] for j in distr.means_]) \n",
        "                GMM_missing.covariances_ = np.array([[j[missing_values] for j in distr.covariances_]])  \n",
        "                \n",
        "                certain_values_true = [not elem for elem in missing_values]  \n",
        "               \n",
        "                GMM_certain = GaussianMixture()\n",
        "                GMM_certain.means_ = np.array([[j[certain_values_true]] for j in distr.means_])\n",
        "                GMM_certain.covariances_ = np.array([[j[certain_values_true] for j in distr.covariances_]])\n",
        "                # print(GMM_certain.covariances_)\n",
        "                # print(\"Missing Means:\", GMM_missing.means_, \"\\n Missing Covs:\", GMM_missing.covariances_)\n",
        "                nv_certain = []\n",
        "                C = regularization(gamma = gamma, n_components = n_components, cov = GMM_certain.covariances_, mu = GMM_certain.means_,x_certain = certain_mean)\n",
        "                for i in range(distr.n_components):\n",
        "                    nv_certain.append(multivariate_normal.pdf(y[certain_values_true], mean = GMM_certain.means_[i][0], cov = np.diagflat(GMM_certain.covariances_[0][i])))\n",
        "                nv_certain = np.array(nv_certain)\n",
        "                # print(\"NV_certain \", nv_certain)\n",
        "                # print(\"Y Values: \", GMM_certain.covariances_[0][4])\n",
        "                # print(\"vor Division: \", distr.weights_, C)\n",
        "                GMM_missing.weights_ = distr.weights_ * C\n",
        "                GMM_missing.weights_ = GMM_missing.weights_/np.sum(GMM_missing.weights_)\n",
        "                # print(\"nach Division: \", GMM_missing.weights_)\n",
        "                GMM_total = GaussianMixture()\n",
        "                dimensions = n_components\n",
        "                GMM_total_mean = []\n",
        "                GMM_total_cov = []\n",
        "                for d in range(0,dimensions):\n",
        "                    GMM_mean_arr = []\n",
        "                    GMM_covariance_arr = []\n",
        "                    counter_gmm = 0  \n",
        "                    counter_arr = 0 \n",
        "                    for i, yi in enumerate(y):\n",
        "                          if np.isnan(yi):\n",
        "                              GMM_mean_arr.append(GMM_missing.means_[d][0][counter_gmm])\n",
        "                              GMM_covariance_arr.append(GMM_missing.covariances_[0][d][counter_gmm])\n",
        "                              counter_gmm+=1\n",
        "                          else:\n",
        "                              GMM_mean_arr.append(certain_mean[counter_arr])\n",
        "                              GMM_covariance_arr.append(certain_cov[counter_arr])\n",
        "                              counter_arr+=1\n",
        "                    #append after every dimension\n",
        "                    GMM_total_mean.append(GMM_mean_arr)\n",
        "                    GMM_total_cov.append(GMM_covariance_arr)\n",
        "                  \n",
        "                matrix_mean = np.array(GMM_total_mean)\n",
        "                matrix_cov = np.array(GMM_total_cov)\n",
        "                GMM_total.means_ = matrix_mean  #np.reshape(matrix_mean, (178, 4))\n",
        "                GMM_total.covariances_ = matrix_cov\n",
        "                GMM_total.weights_ = GMM_missing.weights_\n",
        "                # print(\"Ergebnis von GMM_missing_generation:\", GMM_missing.weights_)\n",
        "\n",
        "                return GMM_total\n",
        "            else:\n",
        "                return distr\n",
        "        else:\n",
        "            return y\n",
        "\n",
        "def fill_gmm(X, df, gamma):\n",
        "    df_distr = df.drop(columns=[\"y\"],axis=1)\n",
        "    distr = fit_distribution(df_distr, components_for_GMM = 5, type_of_distribution=\"GMM\")\n",
        "    GMM_missing= GMM_missing_generation(X, distr, 5, gamma, type_of_distribution=\"GMM\")\n",
        "\n",
        "    return GMM_missing\n",
        "\n",
        "def iterate_gmm_neuron(x, df, n_components, w, b, gamma):\n",
        "    relu_sum = 0\n",
        "    #get distribution\n",
        "    df_distr = df.drop(columns=[\"y\"],axis=1)\n",
        "    distr = fit_distribution(df_distr,n_components,\"GMM\")\n",
        "    #get GMM for missing values\n",
        "    GMM_total = GMM_missing_generation(x,distr,n_components,gamma, type_of_distribution=\"GMM\")\n",
        "    #get Parameters from GMM\n",
        "    mu_arr = GMM_total.means_\n",
        "    #print(mu_arr)\n",
        "    sigma_arr = GMM_total.covariances_\n",
        "    #print(sigma_arr)\n",
        "    p_arr = GMM_total.weights_\n",
        "    print(p_arr)\n",
        "    #iterate through every component\n",
        "    for n in range(0,n_components):\n",
        "        mu = mu_arr[n]\n",
        "        sigma = sigma_arr[n]\n",
        "        p = p_arr[n]\n",
        "        relu_sum += relu_gmm(mu, sigma, w, p, b)\n",
        "    return relu_sum\n",
        "\n",
        "def all_neuron_activated(x,df,n_components, W, B, gamma):\n",
        "    #get distribution\n",
        "    df_distr = df.drop(columns=[\"y\"],axis=1)\n",
        "    distr = fit_distribution(df_distr,n_components,\"GMM\")\n",
        "    activated = []\n",
        "    for neuron_pos in range(0,W.shape[1]):\n",
        "        #get parameters of pretrained network\n",
        "        w = W[:, neuron_pos]\n",
        "        b = B[neuron_pos]\n",
        "        neuron_pos+=1\n",
        "        relu_sum = 0\n",
        "        #get GMM for missing values\n",
        "        GMM_total = GMM_missing_generation(x,distr,n_components,gamma,type_of_distribution=\"GMM\")\n",
        "        #get Parameters from GMM\n",
        "        mu_arr = GMM_total.means_\n",
        "        sigma_arr = GMM_total.covariances_\n",
        "        p_arr = GMM_total.weights_\n",
        "        #iterate through every component\n",
        "        for n in range(0,n_components):\n",
        "            mu = mu_arr[n]\n",
        "            sigma = sigma_arr[n]\n",
        "            p = p_arr[n]\n",
        "            relu_sum += relu_gmm(mu, sigma, w, p, b)\n",
        "        # print(relu_sum)\n",
        "        activated.append(relu_sum)\n",
        "    return np.array(activated)\n",
        "\n",
        "def iterate_data(X,df,n_components, W, B, gamma, model):\n",
        "    predictions = []\n",
        "    df_distr = df.drop(columns=[\"y\"],axis=1)\n",
        "    distr = fit_distribution(df_distr,n_components,\"GMM\")\n",
        "    for index, x in X.iterrows():\n",
        "        activated = []\n",
        "        try:\n",
        "            for neuron_pos in range(0,W.shape[0]):\n",
        "                #get parameters of pretrained network\n",
        "                w = W[neuron_pos]\n",
        "                # print(w)\n",
        "                b = B[neuron_pos]\n",
        "                # print(b)\n",
        "                neuron_pos+=1\n",
        "                relu_sum = 0\n",
        "                #get GMM for missing values\n",
        "                GMM_total = GMM_missing_generation(x,distr,n_components,gamma,type_of_distribution=\"GMM\")\n",
        "                #get Parameters from GMM\n",
        "                # exception (type(GMM_total)) ansonsten mit model.predict normal weiter \n",
        "                # ohne weitere Berechnungen\n",
        "                # Auswertung mit jeweils 100 instanzen bei Customer / IRIS auf vollstÃ¤ndigen\n",
        "                mu_arr = GMM_total.means_\n",
        "                sigma_arr = GMM_total.covariances_\n",
        "                p_arr = GMM_total.weights_\n",
        "                #iterate through every component\n",
        "                for n in range(0,n_components):\n",
        "                    mu = mu_arr[n]\n",
        "                    sigma = sigma_arr[n]\n",
        "                    p = p_arr[n]\n",
        "                    relu_sum += relu_gmm(mu, sigma, w, p, b)\n",
        "                activated.append(relu_sum)\n",
        "    \n",
        "            hidden_activations = np.array(activated)\n",
        "            input_data = hidden_activations.tolist()\n",
        "            # input_data = input_data.reshape(-1, 8)\n",
        "            # predictions.append(new_model.predict(input_data).tolist())\n",
        "            predictions.append(model.propagate(input_data, 1).tolist())\n",
        "        except:\n",
        "            predictions.append(model.propagate(x, 0).tolist())\n",
        "\n",
        "    return predictions \n",
        "\n",
        "\n",
        "def convert_to_one_hot(predictions):\n",
        "    one_hot_pred = []\n",
        "    for i in range(0,len(predictions)):\n",
        "        arr = predictions[i]\n",
        "        max_val = max(arr)\n",
        "        result = [0] * len(arr)\n",
        "        max_index = arr.index(max_val)\n",
        "        result[max_index] = 1\n",
        "        one_hot_pred.append(result)\n",
        "    return one_hot_pred\n"
      ],
      "metadata": {
        "id": "QzY2-xl29kFS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#main\n",
        "if __name__ == \"__main__\":\n",
        "\n",
        "    warnings.filterwarnings('ignore')\n",
        "    \n",
        "    from sklearn.preprocessing import OneHotEncoder\n",
        "    from sklearn.preprocessing import LabelEncoder\n",
        "    sc=StandardScaler()  \n",
        "    le = LabelEncoder()\n",
        "    df = pd.read_csv(\"/content/drive/MyDrive/Lehrstuhl Heinrich/IRIS.csv\")\n",
        "    df.rename(columns = {'species':'y'}, inplace = True)\n",
        "    df[\"y\"] = le.fit_transform(df['y'])    \n",
        "\n",
        "    \n",
        "    \n",
        "    with open('/content/drive/MyDrive/Lehrstuhl Heinrich/iris_test.npy', 'rb') as f:\n",
        "      X = np.load(f)\n",
        "      y = np.load(f)\n",
        "    \n",
        "    percent_delete = 0.4\n",
        "    X_miss = generate_missing_values(X, percent_delete)\n",
        "    X = pd.DataFrame(X_miss)\n",
        "    \n",
        "    model = loadModel('/content/drive/MyDrive/Lehrstuhl Heinrich/model_7_iris_2.pt')\n",
        "\n",
        "    import random\n",
        "    gamma = random.gauss(20, 1)\n",
        "    weights_0 = np.array(model.weights[0])\n",
        "    bias_0 = np.array(model.biases[0])\n",
        "    X = X\n",
        "    n_components = 5\n",
        "    predictions = iterate_data(X, df ,n_components, weights_0, bias_0, gamma, model)\n",
        "\n",
        "    \n",
        "    # Berechne die Anzahl der korrekten Vorhersagen\n",
        "    y_pred = np.array(convert_to_one_hot(predictions))\n",
        "    y_true_encode = np.zeros((y.size, y.max() + 1))\n",
        "    y_true_encode[np.arange(y.size), y] = 1\n",
        "    y_true = y_true_encode\n",
        "    # y_true = np.array(y_true)\n",
        "    print(y_pred)\n",
        "    print(y_true)\n",
        "    # Berechnung der Accuracy\n",
        "    correct_predictions = np.sum(np.all(y_true == y_pred, axis=1))\n",
        "    total_predictions = len(y_true)\n",
        "    accuracy = np.round((correct_predictions / total_predictions),2)\n",
        "    print(\"After deleting: \",percent_delete * 100, \"%, The Accuracy is: \",accuracy)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "jM9Xk-Jj9uWE",
        "outputId": "877bdd8e-0748-44c4-f756-4c02ebd999d9"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[[0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [0 1 0]\n",
            " [0 0 1]\n",
            " [1 0 0]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 0 1]\n",
            " [0 1 0]\n",
            " [1 0 0]\n",
            " [1 0 0]]\n",
            "[[0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [0. 0. 1.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n",
            "After deleting:  40.0 %, The Accuracy is:  0.87\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "g5zXjaCR_EKg"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}